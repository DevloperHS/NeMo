name: &name "Wav2vec_Pretrain"

model:
  train_ds:
    manifest_filepath: ???
    sample_rate: 16000
    batch_size: 32
    shuffle: True
    num_workers: 4
    labels: [ ] # No vocab needed for pre-training
  validation_ds:
    manifest_filepath: ???
    sample_rate: 16000
    batch_size: 32
    shuffle: False
    num_workers: 1
    labels: [ ]
  test_ds:
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 32
    shuffle: False
    num_workers: 4
    labels: [ ]
  optim:
    name: adamw
    lr: 0.0005
    eps: 1e-06
    # optimizer arguments
    betas: [ 0.9, 0.98 ]
    weight_decay: 0.01

    # scheduler setup
    sched:
      name: PolynomialDecayAnnealing
      min_lr: 0.000
      # Scheduler params
      warmup_steps: null
      warmup_ratio: 0.1
trainer:
  gpus: 0 # number of gpus
  num_nodes: 1
  max_epochs: 100
  max_steps: null # computed at runtime if not set
  val_check_interval: 0.5 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  distributed_backend: ddp
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  amp_level: O0 # O1/O2 for mixed precision
  precision: 32 # Should be set to 16 for O1 and O2 to enable the AMP.
  log_every_n_steps: 10 # Interval of logging.
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs
  sync_batchnorm: true

  checkpoint_callback: false # Provided by exp_manager
  logger: false # Provided by exp_manager
exp_manager:
  exp_dir: null
  name: *name
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  create_wandb_logger: false
  wandb_logger_kwargs:
  project: null
  resume_if_exists: false
  resume_ignore_no_checkpoint: false