name: &name "Wav2vec_Pretrain"

model:
  wav2vec:
    loss:
      prob_ppl_weight: 0.1
      feature_loss_weight: 0
    quantize:
      quantize_targets: True
      quantize_input: False
      same_quantizer: False
      latent_vars: 320
      latent_groups: 2
      latent_dim: 0
      latent_temp: [2, 0.5, 0.999995]
    conv_feature_encoder:
      extractor_mode: default
      conv_bias: False
    transformer_encoder:
      dropout: 0.1
      conv:
        conv_pos: 128
        conv_pos_groups: 16
      encoder:
        encoder_layers: 12
        encoder_layerdrop: 0.05
        embedding_dim: 768
        ffn_embedding_dim: 3072
        num_attention_heads: 8
        dropout: 0.1
        activation_fn: gelu
        layer_norm_first: False
    mask:
      mask_prob: 0.65
      mask_type: static
      mask_other: 0
      mask_length: 10
      no_mask_overlap: False
      mask_min_space: 1
      mask_channel_prob: 0
      mask_channel_type: static
      mask_channel_other: 0
      mask_channel_length: 10
      no_mask_channel_overlap: False
      mask_channel_min_space: 1
    dropout_input: 0.1
    dropout_features: 0.1
    final_dim: 0
    n_negatives: 100
    cross_sample_negatives: 0
    codebook_negatives: 0
    negatives_from_everywhere: False
    logit_temp: 0.1
    target_glu: False
    feature_grad_mult: 0.1

  train_ds:
    manifest_filepath: ???
    sample_rate: 16000
    batch_size: 32
    shuffle: True
    num_workers: 4
    labels: [ ] # No vocab needed for pre-training
  validation_ds:
    manifest_filepath: ???
    sample_rate: 16000
    batch_size: 32
    shuffle: False
    num_workers: 1
    labels: [ ]
  test_ds:
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 32
    shuffle: False
    num_workers: 4
    labels: [ ]
  optim:
    name: adamw
    lr: 0.0005
    eps: 1e-06
    # optimizer arguments
    betas: [ 0.9, 0.98 ]
    weight_decay: 0.01

    # scheduler setup
    sched:
      name: PolynomialDecayAnnealing
      min_lr: 0.000
      # Scheduler params
      warmup_steps: null
      warmup_ratio: 0.1
trainer:
  gpus: 0 # number of gpus
  num_nodes: 1
  max_epochs: 100
  max_steps: null # computed at runtime if not set
  val_check_interval: 0.5 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  accelerator: ddp
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  amp_level: O0 # O1/O2 for mixed precision
  precision: 32 # Should be set to 16 for O1 and O2 to enable the AMP.
  log_every_n_steps: 10 # Interval of logging.
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs
  sync_batchnorm: true

  checkpoint_callback: false # Provided by exp_manager
  logger: false # Provided by exp_manager
exp_manager:
  exp_dir: null
  name: *name
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
  resume_if_exists: false
  resume_ignore_no_checkpoint: false